# -*- coding: utf-8 -*-
"""ml2-image_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e0oiMHsp3KOelRePMjc_KCOpXeDsCgDj
"""

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
from matplotlib import pyplot as plt
import math
from google.colab import drive
drive.mount('/content/drive')

class HoughBundler:     
    def __init__(self,min_distance=5,min_angle=2):
        self.min_distance = min_distance
        self.min_angle = min_angle
    
    def get_orientation(self, line):
        orientation = math.atan2(abs((line[3] - line[1])), abs((line[2] - line[0])))
        return math.degrees(orientation)

    def check_is_line_different(self, line_1, groups, min_distance_to_merge, min_angle_to_merge):
        for group in groups:
            for line_2 in group:
                if self.get_distance(line_2, line_1) < min_distance_to_merge:
                    orientation_1 = self.get_orientation(line_1)
                    orientation_2 = self.get_orientation(line_2)
                    if abs(orientation_1 - orientation_2) < min_angle_to_merge:
                        group.append(line_1)
                        return False
        return True

    def distance_point_to_line(self, point, line):
        px, py = point
        x1, y1, x2, y2 = line

        def line_magnitude(x1, y1, x2, y2):
            line_magnitude = math.sqrt(math.pow((x2 - x1), 2) + math.pow((y2 - y1), 2))
            return line_magnitude

        lmag = line_magnitude(x1, y1, x2, y2)
        if lmag < 0.00000001:
            distance_point_to_line = 9999
            return distance_point_to_line

        u1 = (((px - x1) * (x2 - x1)) + ((py - y1) * (y2 - y1)))
        u = u1 / (lmag * lmag)

        if (u < 0.00001) or (u > 1):
            #// closest point does not fall within the line segment, take the shorter distance
            #// to an endpoint
            ix = line_magnitude(px, py, x1, y1)
            iy = line_magnitude(px, py, x2, y2)
            if ix > iy:
                distance_point_to_line = iy
            else:
                distance_point_to_line = ix
        else:
            # Intersecting point is on the line, use the formula
            ix = x1 + u * (x2 - x1)
            iy = y1 + u * (y2 - y1)
            distance_point_to_line = line_magnitude(px, py, ix, iy)

        return distance_point_to_line

    def get_distance(self, a_line, b_line):
        dist1 = self.distance_point_to_line(a_line[:2], b_line)
        dist2 = self.distance_point_to_line(a_line[2:], b_line)
        dist3 = self.distance_point_to_line(b_line[:2], a_line)
        dist4 = self.distance_point_to_line(b_line[2:], a_line)

        return min(dist1, dist2, dist3, dist4)

    def merge_lines_into_groups(self, lines):
        groups = []  # all lines groups are here
        # first line will create new group every time
        groups.append([lines[0]])
        # if line is different from existing gropus, create a new group
        for line_new in lines[1:]:
            if self.check_is_line_different(line_new, groups, self.min_distance, self.min_angle):
                groups.append([line_new])

        return groups

    def merge_line_segments(self, lines):
        orientation = self.get_orientation(lines[0])
      
        if(len(lines) == 1):
            return np.block([[lines[0][:2], lines[0][2:]]])

        points = []
        for line in lines:
            points.append(line[:2])
            points.append(line[2:])
        if 45 < orientation <= 90:
            #sort by y
            points = sorted(points, key=lambda point: point[1])
        else:
            #sort by x
            points = sorted(points, key=lambda point: point[0])

        return np.block([[points[0],points[-1]]])

    def process_lines(self, lines):
        lines_horizontal  = []
        lines_vertical  = []
  
        for line_i in [l[0] for l in lines]:
            orientation = self.get_orientation(line_i)
            # if vertical
            if 45 < orientation <= 90:
                lines_vertical.append(line_i)
            else:
                lines_horizontal.append(line_i)

        lines_vertical  = sorted(lines_vertical , key=lambda line: line[1])
        lines_horizontal  = sorted(lines_horizontal , key=lambda line: line[0])
        merged_lines_all = []

        # for each cluster in vertical and horizantal lines leave only one line
        for i in [lines_horizontal, lines_vertical]:
            if len(i) > 0:
                groups = self.merge_lines_into_groups(i)
                merged_lines = []
                for group in groups:
                    merged_lines.append(self.merge_line_segments(group))
                merged_lines_all.extend(merged_lines)
                    
        return np.asarray(merged_lines_all)

def slope(x1,y1,x2,y2):
    ###finding slope
    if x2!=x1:
        return((y2-y1)/(x2-x1))
    else:
        return 'NA'

def drawLine(image,x1,y1,x2,y2):

    m=slope(x1,y1,x2,y2)
    h,w=image.shape[:2]
    if m!='NA':
        ### here we are essentially extending the line to x=0 and x=width
        ### and calculating the y associated with it
        ##starting point
        px=0
        py=-(x1-0)*m+y1
        ##ending point
        qx=w
        qy=-(x2-w)*m+y2
    else:
    ### if slope is zero, draw a line with x=x1 and y=0 and y=height
        px,py=x1,0
        qx,qy=x1,h
    cv2.line(image, (int(px), int(py)), (int(qx), int(qy)), (255, 255, 255), 1)

def get_contour_areas(contours):
    all_areas = []
    for cnt in contours:
        area= cv2.contourArea(cnt)
        all_areas.append(area)
    return all_areas

def rearrange_points(corners):
  # Find center
  center = [0]*2
  for i in range(corners.shape[0]):
    center[0] += corners[i][0][0]
    center[1] += corners[i][0][1]
  center[0] /= 4
  center[1] /= 4

  rearranged = [None]*4
  for i in range(4):
    if corners[i][0][0] < center[0] and corners[i][0][1] > center[1]:
      rearranged[i] = 0
    elif corners[i][0][0] > center[0] and corners[i][0][1] > center[1]:
      rearranged[i] = 1
    elif corners[i][0][0] > center[0] and corners[i][0][1] < center[1]:
      rearranged[i] = 2
    elif corners[i][0][0] < center[0] and corners[i][0][1] < center[1]:
      rearranged[i] = 3

  corners_copy = [None]*4
  for i in range(4):
    corners_copy[rearranged[i]] = [corners[i][0].tolist()]
  return corners_copy

def getWordContours(image):
  kernel = np.ones((5,5), np.uint8)
  mask = cv2.erode(image, kernel, iterations = 2)
  ret,thresh = cv2.threshold(mask,90,255,cv2.THRESH_BINARY_INV)
  contours, _ = cv2.findContours(image=thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)
  contours_copy = []
  for c in contours:
    if cv2.contourArea(c) < 100:
      continue
    contours_copy.append(c)
  return contours_copy

def getLetterContours(image):
  contours, _ = cv2.findContours(image, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)
  return contours

def flipBit(image):
  copy = np.array(image, copy = True)
  for i in range(len(copy)):
    for j in range(len(copy[i])):
      if copy[i][j] == 255:
        copy[i][j] = 0
      else:
        copy[i][j] = 255
  return copy

def getImage(image, coor):
  return image[coor[1]:coor[1]+coor[3], coor[0]:coor[0]+coor[2]]

img = cv2.imread("/content/image2.jpg")
img_copy = np.array(img, copy = True)

# Convert to graycsale
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# Blur the image for better edge detection
img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)

# Canny Edge Detection
edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200) # Canny Edge Detection
mask = cv2.dilate(edges, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))) 
mask = cv2.GaussianBlur(mask,(5,5),0)

# Find convex hull of contour with largest area
contours, _ = cv2.findContours(image=mask, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)
page = sorted(contours, key=cv2.contourArea, reverse= True)[0]
page = cv2.convexHull(page, False)

# Create a binary image with convex hull
blank_bin = np.zeros_like(img)
cv2.fillPoly(blank_bin, pts =[page], color=(255,255,255))
blank_bin = cv2.cvtColor(blank_bin, cv2.COLOR_BGR2GRAY)

# Canny Edge Detection and contour again to get the outer line of the convex hull
edge_hull = cv2.Canny(image=blank_bin, threshold1=100, threshold2=200)
contours, _ = cv2.findContours(image=edge_hull, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)
blank_copy = np.array(blank_bin, copy = True)
total = 0
for c in contours:
    epsilon = 0.01 * cv2.arcLength(c, True)
    approx = cv2.approxPolyDP(c, epsilon, True)

    cv2.drawContours(blank_copy, [approx], -1, 0, 4)
    total += 1

# Use HoughLinesP to detect straight lines
edge_copy = np.zeros_like(edge_hull)
lines = cv2.HoughLinesP(edge_hull, 1, np.pi/180, 60, minLineLength=10, maxLineGap=250)

# Combine similar lines, then take 4 longest lines as edges for outer quadrilateral of the image
lines = HoughBundler(min_distance=10,min_angle=5).process_lines(lines)
length = np.array([[None]*2 for i in range(lines.shape[0])])
for i in range(len(length)):
  length[i][0] = math.sqrt( (lines[i][0][2]-lines[i][0][0])**2 + (lines[i][0][3] - lines[i][0][1])**2 )
  length[i][1] = i
length_copy = np.flip(length[length[:,0].argsort()])[:4,0]
lines_copy = np.array([None]*4)
for i in range(lines_copy.shape[0]):
  lines_copy[i] = lines[length_copy[i]]
lines = lines_copy

# Extends lines, then use goodFeaturesToTrack to get corners of the quadrilateral, after that use convexHull to rearrange points
edge_copy = np.zeros_like(edges)
for i in range(4):
  drawLine(edge_copy, lines[i][0][0], lines[i][0][1], lines[i][0][2], lines[i][0][3])
corners = cv2.convexHull(np.array(cv2.goodFeaturesToTrack(edge_copy,4,0.01,10), dtype = np.int32), False)
edge_copy = np.zeros_like(edges)

# Fill the convex polygon, find the contours of it
cv2.fillPoly(edge_copy, [corners], color = (255,255,255))
contours, _ = cv2.findContours(image = edge_copy, mode = cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)

# Rearrange and linear transformation to get output cut image
output_size = [860, 540]
corners = np.float32(rearrange_points(corners))
destination = np.float32([[0,output_size[1]], [output_size[0],output_size[1]], [output_size[0],0], [0,0]])
matrix = cv2.getPerspectiveTransform(corners, destination)
output = cv2.warpPerspective(img_copy, matrix, output_size)

# Convert cut to grayscale
img = output
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Get cut section of each information section
information_coor = [[385, 240, 460, 50], [385, 290, 180, 50], [695, 290, 150, 50], [385, 330, 210, 50], [695, 330, 150, 50], [385, 380, 460, 100], [60, 440, 200, 35]]
information_image = [None]*7
for i in range(7):
  information_image[i] = getImage(gray, information_coor[i])

# Get the outer contour of 
information_contours = [None]*7
for i in range(7):
  information_contours[i] = getWordContours(information_image[i])

# Get bounding box of each words
word_bounding_box = [[]for _ in range(7)]
for i in range(7):
  image = np.array(information_image[i], copy = True)
  word_bounding_box[i] = [None]*len(information_contours[i])
  for c in range(len(information_contours[i])):
    word_bounding_box[i][c] = cv2.boundingRect(information_contours[i][c])

# Sort by left to right, top to bottom
for info in range(len(word_bounding_box)):
  y_mean = 0
  for box in word_bounding_box[info]:
    y_mean += box[1]
  y_mean /= len(word_bounding_box[info])
  word_bounding_box[info] = sorted(word_bounding_box[info],key=lambda x: x[0], reverse = False)
  for i in range(len(word_bounding_box[info])):
    if word_bounding_box[info][i][1] > y_mean:
      word_bounding_box[info].append(word_bounding_box[info][i])
      word_bounding_box[info].pop(i)
      i-=1

# Get threshold
word_image = [[]for _ in range(7)]
word_image_thresh = [[]for _ in range(7)]
word_image_thresh_mask = [[]for _ in range(7)]
for i in range(7):
  image = np.array(information_image[i], copy = True)
  for c in range(len(information_contours[i])):
    word_image[i].append(getImage(information_image[i], word_bounding_box[i][c]))
    word_image_thresh[i].append(getImage(information_image[i], word_bounding_box[i][c]))
    word_image_thresh_mask[i].append(getImage(information_image[i], word_bounding_box[i][c]))

for i in range(len(word_image_thresh)):
  for j in range(len(word_image_thresh[i])):
    ret,word_image_thresh[i][j] = cv2.threshold(word_image_thresh[i][j],80,255,cv2.THRESH_BINARY_INV)
    ret,word_image_thresh_mask[i][j] = cv2.threshold(word_image_thresh_mask[i][j],80,255,cv2.THRESH_BINARY_INV)

# Fill every contours to avoid characters' counters
for i in range(len(word_image_thresh_mask)):
  for j in range(len(word_image_thresh_mask[i])):
    contours = getLetterContours(word_image_thresh_mask[i][j])
    for c in contours:
      cv2.fillPoly(word_image_thresh_mask[i][j], [c], color = 255)

# Get a box for every letters
word_box = []
for i in range(len(word_image_thresh_mask)):
  word_box.append([])
  for j in range(len(word_image_thresh_mask[i])):
    word_box[i].append([])
    kernel_horizontal = np.array([[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0]])
    mask = cv2.filter2D(word_image_thresh_mask[i][j], -1, kernel_horizontal)
    for _ in range(20):
      mask = cv2.filter2D(mask, -1, kernel_horizontal)
    contours = getLetterContours(mask)
    box = [None]*len(contours)
    for c in range(len(contours)):
      box[c] = cv2.boundingRect(contours[c])
    box = sorted(box,key=lambda x: x[0], reverse = False)
    for c in range(len(contours)):
      blank_box = np.zeros((50,50))
      box[c] = getImage(word_image_thresh[i][j], box[c])
      h,w = box[c].shape
      blank_box[int(25-h/2):int(25-h/2)+h, int(25-w/2):int(25-w/2)+w] = box[c][0:h, 0:w]
      np.ones((3,3), np.uint8)
      blank_box = flipBit(blank_box)
      word_box[i][j].append(blank_box)